class TestCode(tk.Frame):
    def __init__(self, master, controller, **kwargs):
        super().__init__(master, **kwargs)
        self.controller = controller
        self.base_url = 'https://qdlp2bcmapp0002.ess.fiserv.one/mvcm-api/logs'
        
        # Two-level caching strategy
        self.raw_log_cache = {}  # Cache for raw downloaded logs
        self.filtered_log_cache = {}  # Cache for filtered results
        
        # Create an input frame for date and time filters.
        self.input_frame = ttk.Frame(self)
        self.input_frame.pack(pady=10)
        
        # Date entry label and field.
        ttk.Label(self.input_frame, text="Date (YYYY-MM-DD):").grid(row=0, column=0, padx=5, pady=2)
        self.date_entry = ttk.Entry(self.input_frame)
        self.date_entry.grid(row=0, column=1, padx=5, pady=2)
        
        # Time entry label and field.
        ttk.Label(self.input_frame, text="Time (HH: or HH:MM):").grid(row=1, column=0, padx=5, pady=2)
        self.time_entry = ttk.Entry(self.input_frame)
        self.time_entry.grid(row=1, column=1, padx=5, pady=2)
        
        # Button to start log retrieval.
        self.retrieve_button = ttk.Button(self.input_frame, text="Retrieve Logs", command=self.start_log_retrieval)
        self.retrieve_button.grid(row=2, column=0, columnspan=2, pady=5)

        # Create a panel for displaying logs.
        self.log_panel = ttk.Frame(self)
        self.log_panel.pack(fill='both', expand=True, padx=10, pady=10)
        
        # Create the Text widget for log output.
        self.text_widget = tk.Text(self.log_panel, wrap='none', font=("Verdana", 10))
        self.text_widget.pack(side='left', fill='both', expand=True)
        
        # Create a vertical scrollbar and attach it.
        self.v_scrollbar = ttk.Scrollbar(self.log_panel, orient='vertical', command=self.text_widget.yview)
        self.v_scrollbar.pack(side='right', fill='y')
        self.text_widget.configure(yscrollcommand=self.v_scrollbar.set)
        
        # Add cache status display
        ttk.Label(self.input_frame, text="Cache Status:").grid(row=3, column=0, padx=5, pady=2)
        self.cache_status = ttk.Label(self.input_frame, text="0 logs cached")
        self.cache_status.grid(row=3, column=1, padx=5, pady=2)
        
        # Add clear cache button
        self.clear_cache_button = ttk.Button(self.input_frame, text="Clear Cache", command=self.clear_cache)
        self.clear_cache_button.grid(row=4, column=0, columnspan=2, pady=5)
    
    def clear_cache(self):
        """Clears both raw and filtered log caches."""
        self.raw_log_cache = {}
        self.filtered_log_cache = {}
        self.update_cache_status()
        self.text_widget.delete("1.0", tk.END)
        self.text_widget.insert(tk.END, "Cache cleared.\n")
    
    def update_cache_status(self):
        """Updates the cache status display."""
        self.cache_status.config(text=f"{len(self.raw_log_cache)} logs cached")
    
    def start_log_retrieval(self):
        """Starts a background thread to retrieve logs based on the entered date and time filters."""
        date_filter = self.date_entry.get().strip()
        time_filter = self.time_entry.get().strip()
        
        # If fields are empty, use None so that no filtering is applied.
        if date_filter == "":
            date_filter = None
        if time_filter == "":
            time_filter = None

        # Clear the text widget before starting the search
        self.text_widget.delete("1.0", tk.END)
        self.text_widget.insert(tk.END, "Starting log retrieval...\n")
        
        # Start a background thread so the UI remains responsive.
        retrieval_thread = threading.Thread(target=self.testCode, args=(date_filter, time_filter))
        retrieval_thread.daemon = True
        retrieval_thread.start()
    
    def log_timing(self, message):
        """Log timing information to the text widget on the main thread."""
        self.text_widget.after(0, lambda: self.text_widget.insert(tk.END, f"{message}\n"))
        
    def get_or_download_log(self, package_index, parent, name):
        """
        Gets a log from cache or downloads it if not cached.
        Returns the raw log content.
        """
        # Construct the download URL
        if parent and not (name.startswith('server.') and name.endswith('.log') and name[7:-4].isdigit()) and not (name.startswith('catalina.')):
            download_url = f'{self.base_url}/download/{package_index}/{parent}/{name}'
        else:
            download_url = f'{self.base_url}/download/{package_index}/{name}'
        
        # Cache key for raw log
        cache_key = download_url
        
        # Check raw log cache first
        if cache_key in self.raw_log_cache:
            return self.raw_log_cache[cache_key], 0  # Return cached content and 0 download time
        
        # Download the log file if not in cache
        start_time = time.time()
        download_resp = self.controller.mvcm.getalllogs(download_url)
        download_time = time.time() - start_time
        
        log_content = download_resp.text if hasattr(download_resp, 'text') else download_resp
        
        # Cache the raw log
        self.raw_log_cache[cache_key] = log_content
        self.update_cache_status()
        
        return log_content, download_time
    
    def get_filtered_log(self, raw_log, time_filter, date_filter):
        """
        Filters a log based on time and date filters.
        First checks if this filter has been applied before (in filtered_log_cache).
        """
        # If no filters, return raw log
        if not time_filter and (not date_filter or date_filter == "0000-00-00"):
            return raw_log, 0
        
        # Create a unique key for this raw log and filter combination
        # We'll use a hash of the raw log as part of the key to avoid extremely long keys
        raw_log_hash = hash(raw_log)
        filter_key = f"{raw_log_hash}_{time_filter}_{date_filter}"
        
        # Check filtered log cache
        if filter_key in self.filtered_log_cache:
            return self.filtered_log_cache[filter_key], 0
        
        # Apply filter and time it
        start_time = time.time()
        filtered_log = self.filter_log_content(raw_log, time_filter, date_filter)
        filter_time = time.time() - start_time
        
        # Cache the filtered result
        self.filtered_log_cache[filter_key] = filtered_log
        
        return filtered_log, filter_time
    
    def filter_log_content(self, log_content, time_filter, date_filter):
        """
        Filters log content based on time and date filters.
        This is separated for better profiling and optimization.
        """
        # Shortcut: if both filters are None, return the original content
        if not time_filter and (not date_filter or date_filter == "0000-00-00"):
            return log_content
            
        # Parse time filter
        desired_hour = None
        desired_minute = None
        desired_tenth = None
        
        if time_filter:
            time_parts = time_filter.split(":")
            desired_hour = time_parts[0]
            if len(time_parts) > 1:
                desired_minute = time_parts[1]
                if len(desired_minute) == 1:
                    desired_tenth = desired_minute[0]
                if desired_minute == '':
                    desired_minute = None
        
        desired_date = date_filter if date_filter and date_filter != "0000-00-00" else None
        
        # Create a regular expression pattern for faster filtering
        if desired_date and desired_hour and desired_minute:
            # Full date and time (YYYY-MM-DD HH:MM)
            pattern = f"{desired_date} {desired_hour}:{desired_minute}"
            filtered_lines = [line for line in log_content.splitlines() if len(line) >= 19 and line.startswith(pattern)]
            print(pattern, filtered_lines)
        elif desired_date and desired_hour and desired_tenth:
            # Date, hour and tenth of minute (YYYY-MM-DD HH:M)
            pattern = f"{desired_date} {desired_hour}:{desired_tenth}"
            filtered_lines = [line for line in log_content.splitlines() if len(line) >= 19 and line.startswith(pattern)]
        elif desired_date and desired_hour:
            # Date and hour only (YYYY-MM-DD HH)
            pattern = f"{desired_date} {desired_hour}:"
            filtered_lines = [line for line in log_content.splitlines() if len(line) >= 19 and line.startswith(pattern)]
        elif desired_date:
            # Date only (YYYY-MM-DD)
            filtered_lines = [line for line in log_content.splitlines() if len(line) >= 10 and line.startswith(desired_date)]
        elif desired_hour and desired_minute:
            # Hour and minute only (HH:MM)
            pattern = f"{desired_hour}:{desired_minute}"
            filtered_lines = [line for line in log_content.splitlines() if len(line) >= 19 and pattern in line[:16]]
        elif desired_hour and desired_tenth:
            # Hour and tenth of minute (HH:M)
            pattern = f"{desired_hour}:{desired_tenth}"
            filtered_lines = [line for line in log_content.splitlines() if len(line) >= 19 and pattern in line[:15]]
        elif desired_hour:
            # Hour only (HH)
            pattern = f"{desired_hour}:"
            filtered_lines = [line for line in log_content.splitlines() if len(line) >= 19 and pattern in line[:14]]
        else:
            # No valid filters, return the original content
            return log_content
        
        return "\n".join(filtered_lines)

    def download_and_filter_log(self, package_index, parent, name, time_filter, date_filter):
        """
        Gets a log (from cache or by downloading) and applies filtering.
        Returns a tuple of (filtered_log_content, download_time, filter_time)
        """
        # First, get or download the raw log
        raw_log, download_time = self.get_or_download_log(package_index, parent, name)
        
        # Then filter the log (from filter cache or by applying filter)
        filtered_log, filter_time = self.get_filtered_log(raw_log, time_filter, date_filter)
        
        return filtered_log, download_time, filter_time

    def testCode(self, specified_date=None, specified_time=None):
        """
        Retrieve log files from the API with optional date and time filtering.
        """
        overall_start_time = time.time()
        
        # Set default date filter if none provided
        date_filter = specified_date if specified_date else "0000-00-00"
        time_filter = specified_time if specified_time else ""
        
        self.log_timing(f"Starting log retrieval with date filter: {date_filter}, time filter: {time_filter}")
        
        # Fetch packages list
        packages_start_time = time.time()
        resp = self.controller.mvcm.getalllogs(self.base_url)
        packages_end_time = time.time()
        packages_time = packages_end_time - packages_start_time
        self.log_timing(f"API call for packages list took: {packages_time:.3f} seconds")
        
        try:
            packages = resp.json()
        except Exception:
            packages = json.loads(resp.text)
        
        # Collect all logs to be downloaded
        logs_to_download = []
        log_list_fetch_time = 0
        
        for i, package in enumerate(packages, 1):
            log_type = package.get('name', 'Unknown')
            
            list_start_time = time.time()
            resp = self.controller.mvcm.getalllogs(f'{self.base_url}/{i}')
            list_end_time = time.time()
            log_list_fetch_time += list_end_time - list_start_time
            
            log_files_array = resp.json().get('logFiles', [])
            for log_file in log_files_array:
                name = log_file.get('name', 'Unnamed Log')
                parent = log_file.get('parent')
                date_modified = log_file.get('dateModified')
                
                # Only process logs modified on or after the specified date
                if not date_filter or date_filter == "0000-00-00" or (date_modified and date_modified >= date_filter):
                    logs_to_download.append((i, parent, name, log_type))
        
        self.log_timing(f"API calls for log lists took: {log_list_fetch_time:.3f} seconds")
        self.log_timing(f"Found {len(logs_to_download)} logs to process")
        
        # Track cache statistics
        cache_hits = 0
        cache_misses = 0
        
        # Use ThreadPoolExecutor to parallelize the API calls for downloading logs
        log_files = {}
        total_download_time = 0
        total_filter_time = 0
        
        with ThreadPoolExecutor(max_workers=10) as executor:
            future_to_log = {}
            for (package_index, parent, name, log_type) in logs_to_download:
                future = executor.submit(
                    self.download_and_filter_log, 
                    package_index, parent, name, time_filter, date_filter
                )
                future_to_log[future] = (name, log_type)
            
            # Collect results as they complete
            for future in concurrent.futures.as_completed(future_to_log):
                name, log_type = future_to_log[future]
                try:
                    log_content, download_time, filter_time = future.result()
                    
                    # If download_time is 0, it was a cache hit
                    if download_time == 0:
                        cache_hits += 1
                    else:
                        cache_misses += 1
                        total_download_time += download_time
                        
                    total_filter_time += filter_time
                    
                    if log_content and log_content.strip():  # Only keep non-empty logs
                        log_files[name] = {'type': log_type, 'content': log_content}
                except Exception as exc:
                    self.log_timing(f"Error retrieving log {name}: {exc}")
        
        overall_end_time = time.time()
        overall_time = overall_end_time - overall_start_time
        
        # Summary timing information
        self.log_timing(f"Cache hits: {cache_hits}, Cache misses: {cache_misses}")
        self.log_timing(f"Total download time: {total_download_time:.3f} seconds")
        self.log_timing(f"Total filtering time: {total_filter_time:.3f} seconds")
        self.log_timing(f"Overall processing time: {overall_time:.3f} seconds")
        self.log_timing(f"Retrieved {len(log_files)} logs with content")

        print(f"Cache hits: {cache_hits}, Cache misses: {cache_misses}")
        print(f"Total download time: {total_download_time:.3f} seconds")
        print(f"Total filtering time: {total_filter_time:.3f} seconds")
        print(f"Overall processing time: {overall_time:.3f} seconds")
        print(f"Retrieved {len(log_files)} logs with content")
        
        # Schedule the UI update on the main thread
        self.text_widget.after(0, self.update_text_widget, log_files)
    
    def update_text_widget(self, log_files):
        """Update the Text widget with the retrieved logs."""
        self.text_widget.delete("1.0", tk.END)
        self.text_widget.insert(tk.END, "Collected Log Files:\n")
        self.text_widget.insert(tk.END, "=" * 80 + "\n")
        
        #if not log_files:
        #    self.text_widget.insert(tk.END, "No logs found matching the criteria.\n")
        #    return
            
        for log_name, details in log_files.items():
            if not details['content']:
                continue
                
            self.text_widget.insert(tk.END, f"Log Name: {log_name}\n")
            self.text_widget.insert(tk.END, f"Log Type: {details['type']}\n")
            self.text_widget.insert(tk.END, "-" * 80 + "\n")
            self.text_widget.insert(tk.END, details['content'] + "\n")
            self.text_widget.insert(tk.END, "=" * 80 + "\n")
